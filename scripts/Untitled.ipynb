{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24472a6f-00a2-44ba-bd85-cca283a34743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-07-09 12:02:45,959][INFO]: Connecting pfirouzabadi@127.0.0.1:3306\n",
      "[2024-07-09 12:02:46,059][INFO]: Connected pfirouzabadi@127.0.0.1:3306\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='6'\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'osmesa'\n",
    "import torch\n",
    "from pose_pipeline.utils.jupyter import play,play_grid\n",
    "from pose_pipeline.pipeline import BlurredVideo,LiftingPerson,LiftingMethod,TopDownPerson,Video,VideoInfo\n",
    "import logging\n",
    "import mimetypes\n",
    "import time\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import cv2\n",
    "import json_tricks as json\n",
    "import mmcv\n",
    "import mmengine\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7177cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_camera.datajoint.multi_camera_dj import PersonKeypointReconstruction,SingleCameraVideo, CalibratedRecording, MultiCameraRecording,PersonKeypointReconstructionMethod\n",
    "from multi_camera.datajoint.sessions import Recording\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a06d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(0.5, 0)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce18850-9d41-4e1b-a3a4-00e0a033f9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import \\\n"
     ]
    }
   ],
   "source": [
    "from mmpose.apis import inference_topdown, init_model\n",
    "from mmpose.apis import init_model as init_pose_estimator\n",
    "from mmpose.evaluation.functional import nms\n",
    "from mmpose.registry import VISUALIZERS\n",
    "from mmpose.structures import merge_data_samples, split_instances\n",
    "from mmpose.utils import adapt_mmdet_pipeline\n",
    "import cv2\n",
    "try:\n",
    "    from mmdet.apis import inference_detector, init_detector\n",
    "    has_mmdet = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    has_mmdet = False\n",
    "\n",
    "\n",
    "from mmcv.image import imread\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection and estimation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_detector_model(detection_cfg, detection_ckpt,device):\n",
    "    # build detector\n",
    "    detector = init_detector(\n",
    "        detection_cfg, detection_ckpt, device=device)\n",
    "    detector.cfg = adapt_mmdet_pipeline(detector.cfg)\n",
    "    return detector\n",
    "def init_pose_model(pose_model_cfg, pose_model_ckpt,device):\n",
    "    pose_estimator = init_model(pose_model_cfg, pose_model_ckpt, device=device)\n",
    "    pose_estimator.cfg.visualizer.radius = 3\n",
    "    pose_estimator.cfg.visualizer.alpha = 0.7\n",
    "    pose_estimator.cfg.visualizer.line_width = 1\n",
    "    \n",
    "    return pose_estimator\n",
    "\n",
    "def init_visualizer(pose_estimator):\n",
    "    # build visualizer\n",
    "    visualizer = VISUALIZERS.build(pose_estimator.cfg.visualizer)\n",
    "    # the dataset_meta is loaded from the checkpoint and\n",
    "    # then pass to the model in init_pose_estimator\n",
    "    visualizer.set_dataset_meta(\n",
    "        pose_estimator.dataset_meta)\n",
    "    return visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e27862",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# alg_name = \"Freihand\"\n",
    "# pose_model_cfg = './configs/hand_2d_keypoint/topdown_heatmap/freihand2d/td-hm_res50_8xb64-100e_freihand2d-224x224.py'\n",
    "# pose_model_ckpt = 'https://download.openmmlab.com/mmpose/hand/resnet/res50_freihand_224x224-ff0799bc_20200914.pth'\n",
    "\n",
    "# alg_name = \"HRNet_dark\"\n",
    "# pose_model_cfg = \"./configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_hrnetv2-w18_dark-8xb64-210e_rhd2d-256x256.py\"\n",
    "# pose_model_ckpt = \"https://download.openmmlab.com/mmpose/hand/dark/hrnetv2_w18_rhd2d_256x256_dark-4df3a347_20210330.pth\"\n",
    "# alg_name = \"RTMPoseCOCO\"\n",
    "# pose_model_cfg = './configs/hand_2d_keypoint/rtmpose/coco_wholebody_hand/rtmpose-m_8xb32-210e_coco-wholebody-hand-256x256.py'\n",
    "# pose_model_ckpt = 'https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmpose-m_simcc-coco-wholebody-hand_pt-aic-coco_210e-256x256-99477206_20230228.pth'\n",
    "\n",
    "alg_name = \"RTMPoseHand5\"\n",
    "pose_model_cfg = '/home/pfirouzabadi/projects/Hand_Detection/hand_detection/wrappers/mmpose/configs/hand_2d_keypoint/rtmpose/hand5/rtmpose-m_8xb256-210e_hand5-256x256.py'\n",
    "pose_model_ckpt = 'https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmpose-m_simcc-hand5_pt-aic-coco_210e-256x256-74fb594_20230320.pth'\n",
    "device = 'cuda'\n",
    "\n",
    "detection_cfg = '/home/pfirouzabadi/projects/Hand_Detection/hand_detection/wrappers/mmpose/demo/mmdetection_cfg/rtmdet_nano_320-8xb32_hand.py'\n",
    "detection_ckpt = 'https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmdet_nano_8xb32-300e_hand-267f9c8f.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98bde000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    <style type=\"text/css\">\n",
       "        .Table{\n",
       "            border-collapse:collapse;\n",
       "        }\n",
       "        .Table th{\n",
       "            background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid;\n",
       "            font-weight: normal; font-family: monospace; font-size: 100%;\n",
       "        }\n",
       "        .Table td{\n",
       "            padding:4px; border:#f0e0e0 1px solid; font-size:100%;\n",
       "        }\n",
       "        .Table tr:nth-child(odd){\n",
       "            background: #ffffff;\n",
       "            color: #000000;\n",
       "        }\n",
       "        .Table tr:nth-child(even){\n",
       "            background: #f3f1ff;\n",
       "            color: #000000;\n",
       "        }\n",
       "        /* Tooltip container */\n",
       "        .djtooltip {\n",
       "        }\n",
       "        /* Tooltip text */\n",
       "        .djtooltip .djtooltiptext {\n",
       "            visibility: hidden;\n",
       "            width: 120px;\n",
       "            background-color: black;\n",
       "            color: #fff;\n",
       "            text-align: center;\n",
       "            padding: 5px 0;\n",
       "            border-radius: 6px;\n",
       "            /* Position the tooltip text - see examples below! */\n",
       "            position: absolute;\n",
       "            z-index: 1;\n",
       "        }\n",
       "        #primary {\n",
       "            font-weight: bold;\n",
       "            color: black;\n",
       "        }\n",
       "        #nonprimary {\n",
       "            font-weight: normal;\n",
       "            color: white;\n",
       "        }\n",
       "\n",
       "        /* Show the tooltip text when you mouse over the tooltip container */\n",
       "        .djtooltip:hover .djtooltiptext {\n",
       "            visibility: visible;\n",
       "        }\n",
       "    </style>\n",
       "    \n",
       "    <b>Single video from a multi-camera recording</b>\n",
       "        <div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "        <table border=\"1\" class=\"Table\">\n",
       "            <thead> <tr style=\"text-align: right;\"> <th> <div class=\"djtooltip\">\n",
       "                            <p id=\"primary\">recording_timestamps</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"primary\">camera_config_hash</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"primary\">video_project</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"primary\">filename</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">camera_name</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">frame_timestamps</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div> </th> </tr> </thead>\n",
       "            <tbody> <tr> <td>2024-06-14 12:48:23</td>\n",
       "<td>39932d9e29</td>\n",
       "<td>HAND_TEST</td>\n",
       "<td>m001_disk_20240614_124823.23015083</td>\n",
       "<td>23015083</td>\n",
       "<td>=BLOB=</td></tr><tr><td>2024-06-14 12:48:23</td>\n",
       "<td>39932d9e29</td>\n",
       "<td>HAND_TEST</td>\n",
       "<td>m001_disk_20240614_124823.23015089</td>\n",
       "<td>23015089</td>\n",
       "<td>=BLOB=</td></tr><tr><td>2024-06-14 12:48:23</td>\n",
       "<td>39932d9e29</td>\n",
       "<td>HAND_TEST</td>\n",
       "<td>m001_disk_20240614_124823.23106526</td>\n",
       "<td>23106526</td>\n",
       "<td>=BLOB=</td></tr><tr><td>2024-06-14 12:48:23</td>\n",
       "<td>39932d9e29</td>\n",
       "<td>HAND_TEST</td>\n",
       "<td>m001_disk_20240614_124823.23106528</td>\n",
       "<td>23106528</td>\n",
       "<td>=BLOB=</td></tr><tr><td>2024-06-14 12:48:23</td>\n",
       "<td>39932d9e29</td>\n",
       "<td>HAND_TEST</td>\n",
       "<td>m001_disk_20240614_124823.23106530</td>\n",
       "<td>23106530</td>\n",
       "<td>=BLOB=</td></tr><tr><td>2024-06-14 12:48:23</td>\n",
       "<td>39932d9e29</td>\n",
       "<td>HAND_TEST</td>\n",
       "<td>m001_disk_20240614_124823.23106542</td>\n",
       "<td>23106542</td>\n",
       "<td>=BLOB=</td></tr><tr><td>2024-06-14 12:48:23</td>\n",
       "<td>39932d9e29</td>\n",
       "<td>HAND_TEST</td>\n",
       "<td>m001_disk_20240614_124823.23106544</td>\n",
       "<td>23106544</td>\n",
       "<td>=BLOB=</td></tr><tr><td>2024-06-14 12:48:23</td>\n",
       "<td>39932d9e29</td>\n",
       "<td>HAND_TEST</td>\n",
       "<td>m001_disk_20240614_124823.23336088</td>\n",
       "<td>23336088</td>\n",
       "<td>=BLOB=</td> </tr> </tbody>\n",
       "        </table>\n",
       "        \n",
       "        <p>Total: 8</p></div>\n",
       "        "
      ],
      "text/plain": [
       "*recording_tim *camera_config *video_project *filename      camera_name    frame_time\n",
       "+------------+ +------------+ +------------+ +------------+ +------------+ +--------+\n",
       "2024-06-14 12: 39932d9e29     HAND_TEST      m001_disk_2024 23015083       =BLOB=    \n",
       "2024-06-14 12: 39932d9e29     HAND_TEST      m001_disk_2024 23015089       =BLOB=    \n",
       "2024-06-14 12: 39932d9e29     HAND_TEST      m001_disk_2024 23106526       =BLOB=    \n",
       "2024-06-14 12: 39932d9e29     HAND_TEST      m001_disk_2024 23106528       =BLOB=    \n",
       "2024-06-14 12: 39932d9e29     HAND_TEST      m001_disk_2024 23106530       =BLOB=    \n",
       "2024-06-14 12: 39932d9e29     HAND_TEST      m001_disk_2024 23106542       =BLOB=    \n",
       "2024-06-14 12: 39932d9e29     HAND_TEST      m001_disk_2024 23106544       =BLOB=    \n",
       "2024-06-14 12: 39932d9e29     HAND_TEST      m001_disk_2024 23336088       =BLOB=    \n",
       " (Total: 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_keys = (SingleCameraVideo & 'filename LIKE \"m001%disk_2024%\"').fetch('KEY')\n",
    "# (Video & vid_keys[::8][5]).fetch('video')\n",
    "# keys= (SingleCameraVideo & vid_keys[5::8]).fetch('KEY')\n",
    "# keys = np.array(keys)\n",
    "# (Video &  keys[[4,1,3]]).fetch('video')\n",
    "\n",
    "SingleCameraVideo & vid_keys#[[4,1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "903cd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = (VideoInfo & keys[[4,1,3]]).fetch('timestamps')\n",
    "np.save('timestamps.npy',arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d884770",
   "metadata": {},
   "outputs": [],
   "source": [
    "(VideoInfo.populate(keys[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f838b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "info_key = (VideoInfo & (SingleCameraVideo & vid_keys)).fetch(\"KEY\")[0]\n",
    "timestamps = (VideoInfo & info_key).fetch_timestamps()[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74b5e937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    \n",
       "    <style type=\"text/css\">\n",
       "        .Table{\n",
       "            border-collapse:collapse;\n",
       "        }\n",
       "        .Table th{\n",
       "            background: #A0A0A0; color: #ffffff; padding:4px; border:#f0e0e0 1px solid;\n",
       "            font-weight: normal; font-family: monospace; font-size: 100%;\n",
       "        }\n",
       "        .Table td{\n",
       "            padding:4px; border:#f0e0e0 1px solid; font-size:100%;\n",
       "        }\n",
       "        .Table tr:nth-child(odd){\n",
       "            background: #ffffff;\n",
       "            color: #000000;\n",
       "        }\n",
       "        .Table tr:nth-child(even){\n",
       "            background: #f3f1ff;\n",
       "            color: #000000;\n",
       "        }\n",
       "        /* Tooltip container */\n",
       "        .djtooltip {\n",
       "        }\n",
       "        /* Tooltip text */\n",
       "        .djtooltip .djtooltiptext {\n",
       "            visibility: hidden;\n",
       "            width: 120px;\n",
       "            background-color: black;\n",
       "            color: #fff;\n",
       "            text-align: center;\n",
       "            padding: 5px 0;\n",
       "            border-radius: 6px;\n",
       "            /* Position the tooltip text - see examples below! */\n",
       "            position: absolute;\n",
       "            z-index: 1;\n",
       "        }\n",
       "        #primary {\n",
       "            font-weight: bold;\n",
       "            color: black;\n",
       "        }\n",
       "        #nonprimary {\n",
       "            font-weight: normal;\n",
       "            color: white;\n",
       "        }\n",
       "\n",
       "        /* Show the tooltip text when you mouse over the tooltip container */\n",
       "        .djtooltip:hover .djtooltiptext {\n",
       "            visibility: visible;\n",
       "        }\n",
       "    </style>\n",
       "    \n",
       "    <b>Video info including timestamps, delta times, num frames, height and width</b>\n",
       "        <div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "        <table border=\"1\" class=\"Table\">\n",
       "            <thead> <tr style=\"text-align: right;\"> <th> <div class=\"djtooltip\">\n",
       "                            <p id=\"primary\">video_project</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"primary\">filename</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">timestamps</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">delta_time</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">fps</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">height</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">width</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div></th><th><div class=\"djtooltip\">\n",
       "                            <p id=\"nonprimary\">num_frames</p>\n",
       "                            <span class=\"djtooltiptext\"></span>\n",
       "                        </div> </th> </tr> </thead>\n",
       "            <tbody> <tr> <td>HAND_TEST</td>\n",
       "<td>m001_disk_force_20240614_134221.23106542</td>\n",
       "<td>=BLOB=</td>\n",
       "<td>=BLOB=</td>\n",
       "<td>29.08</td>\n",
       "<td>1536</td>\n",
       "<td>2048</td>\n",
       "<td>1847</td> </tr> </tbody>\n",
       "        </table>\n",
       "        \n",
       "        <p>Total: 1</p></div>\n",
       "        "
      ],
      "text/plain": [
       "*video_project *filename      timestamps delta_time fps       height     width     num_frames    \n",
       "+------------+ +------------+ +--------+ +--------+ +-------+ +--------+ +-------+ +------------+\n",
       "HAND_TEST      m001_disk_forc =BLOB=     =BLOB=     29.08     1536       2048      1847          \n",
       " (Total: 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VideoInfo & keys[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd966c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_idx = 0\n",
    "participant_videos = ((PersonKeypointReconstruction & (Recording & 'participant_id=\"m002\"') & 'top_down_method=2' & 'reconstruction_method=0').fetch('KEY')[participant_idx])\n",
    "vid_keys= (TopDownPerson * MultiCameraRecording * SingleCameraVideo & participant_videos).fetch('KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc896a7c",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmdet_nano_8xb32-300e_hand-267f9c8f.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pfirouzabadi/projects/Hand_Detection/hand_detection/wrappers/mmpose/mmpose/datasets/datasets/utils.py:102: UserWarning: The metainfo config file \"configs/_base_/datasets/onehand10k.py\" does not exist. A matched config file \"/home/pfirouzabadi/projects/Hand_Detection/hand_detection/wrappers/mmpose/mmpose/.mim/configs/_base_/datasets/onehand10k.py\" will be used instead.\n",
      "  warnings.warn(\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/apis/inference.py:108: UserWarning: palette does not exist, random is used by default. You can also set the palette to customize.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmpose-m_simcc-hand5_pt-aic-coco_210e-256x256-74fb594_20230320.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/layers/se_layer.py:158: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/backbones/csp_darknet.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/layers/se_layer.py:158: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/backbones/csp_darknet.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/layers/se_layer.py:158: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/backbones/csp_darknet.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/layers/se_layer.py:158: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/backbones/csp_darknet.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/layers/se_layer.py:158: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/backbones/csp_darknet.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/layers/se_layer.py:158: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/backbones/csp_darknet.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/layers/se_layer.py:158: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/backbones/csp_darknet.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/layers/se_layer.py:158: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/backbones/csp_darknet.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    }
   ],
   "source": [
    "detector = init_detector_model(detection_cfg=detection_cfg,detection_ckpt=detection_ckpt,device='cpu')\n",
    "pose_estimator = init_pose_model(pose_model_cfg=pose_model_cfg, pose_model_ckpt=pose_model_ckpt, device=device)\n",
    "visualizer = init_visualizer(pose_estimator)\n",
    "\n",
    "output_root = f'./visualizations/{alg_name}/'\n",
    "if not os.path.exists(output_root):\n",
    "    os.mkdir(output_root)\n",
    "\n",
    "#RUN VIDEOS FROM SPECIFIC VIEWS\n",
    "for vid_idx in range(len(vid_keys)):\n",
    "    vid_file = (Video &vid_keys[vid_idx]).fetch1('video')\n",
    "    vid_file_prefix=  ''.join((SingleCameraVideo & vid_keys[vid_idx]).fetch1('filename').split('_')[:2])\n",
    "    output_file = f'{output_root}{vid_file_prefix}_camera_{vid_idx}.mp4'\n",
    "    \n",
    "    cap = cv2.VideoCapture(vid_file)\n",
    "    frame_idx = 0\n",
    "    video_writer = None\n",
    "    pred_instances_list = []\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        frame_idx += 1\n",
    "\n",
    "        if not success:\n",
    "            break\n",
    "        # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        det_result = inference_detector(detector, frame)\n",
    "        pred_instance = det_result.pred_instances.cpu().numpy()\n",
    "        bboxes = np.concatenate(\n",
    "            (pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)\n",
    "        \n",
    "        bboxes = bboxes[np.logical_and(pred_instance.labels == 0,\n",
    "                                        pred_instance.scores > .2)]\n",
    "        bboxes = bboxes[nms(bboxes, .2), :4]\n",
    "        # bboxes[:,:2] -= 50\n",
    "        # bboxes[:,-2:] += 50\n",
    "\n",
    "        \n",
    "        # predict keypoints\n",
    "        pose_results = inference_topdown(pose_estimator, frame, bboxes)\n",
    "        data_samples = merge_data_samples(pose_results)\n",
    "\n",
    "        visualizer.add_datasample(\n",
    "            'result',\n",
    "            frame,\n",
    "            data_sample=data_samples,\n",
    "            draw_gt=False,\n",
    "            draw_heatmap=True,\n",
    "            draw_bbox=True,\n",
    "            show_kpt_idx=True,\n",
    "            # show=args.show,\n",
    "            wait_time=0,\n",
    "            kpt_thr=0.3)\n",
    "        # topdown pose estimation\n",
    "        pred_instances = data_samples.get('pred_instances', None)\n",
    "\n",
    "        # save prediction results\n",
    "        pred_instances_list.append(\n",
    "            dict(\n",
    "                frame_id=frame_idx,\n",
    "                instances=split_instances(pred_instances)))\n",
    "\n",
    "        # output videos\n",
    "        frame_vis = visualizer.get_image()\n",
    "\n",
    "        if video_writer is None:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            # the size of the image with visualization may vary\n",
    "            # depending on the presence of heatmaps\n",
    "            video_writer = cv2.VideoWriter(\n",
    "                output_file,\n",
    "                fourcc,\n",
    "                25,  # saved fps\n",
    "                (frame_vis.shape[1], frame_vis.shape[0]))\n",
    "        if frame_idx%300 == 0:\n",
    "            ress=cv2.imwrite(f'{output_file[:-4]}_frame{frame_idx}.jpeg',frame_vis)\n",
    "\n",
    "        video_writer.write(frame_vis)\n",
    "\n",
    "        # if args.show:\n",
    "        #     # press ESC to exit\n",
    "        #     if cv2.waitKey(5) & 0xFF == 27:\n",
    "        #         break\n",
    "\n",
    "        #     time.sleep(args.show_interval)\n",
    "\n",
    "    if video_writer:\n",
    "        video_writer.release()\n",
    "\n",
    "    cap.release()\n",
    "    os.remove(vid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "139d073e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CyberGlove'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "folder = \"/home/pfirouzabadi/projects/Hand_Detection/data/Gloves/*.mp4\"\n",
    "paths = glob.glob(folder)\n",
    "''.join(paths[0].split('/')[-1].split('.')[0].split('_')[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "777fd59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/09 12:08:17 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The current default scope \"mmpose\" is not \"mmdet\", `init_default_scope` will force set the currentdefault scope to \"mmdet\".\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmdet_nano_8xb32-300e_hand-267f9c8f.pth\n",
      "07/09 12:08:17 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The current default scope \"mmdet\" is not \"mmpose\", `init_default_scope` will force set the currentdefault scope to \"mmpose\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pfirouzabadi/projects/Hand_Detection/hand_detection/wrappers/mmpose/mmpose/datasets/datasets/utils.py:102: UserWarning: The metainfo config file \"configs/_base_/datasets/onehand10k.py\" does not exist. A matched config file \"/home/pfirouzabadi/projects/Hand_Detection/hand_detection/wrappers/mmpose/mmpose/.mim/configs/_base_/datasets/onehand10k.py\" will be used instead.\n",
      "  warnings.warn(\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/apis/inference.py:108: UserWarning: palette does not exist, random is used by default. You can also set the palette to customize.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmpose-m_simcc-hand5_pt-aic-coco_210e-256x256-74fb594_20230320.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmengine/utils/manager.py:113: UserWarning: <class 'mmpose.visualization.local_visualizer.PoseLocalVisualizer'> instance named of visualizer has been created, the method `get_instance` should not accept any other arguments\n",
      "  warnings.warn(\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/layers/se_layer.py:158: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/mmdet/models/backbones/csp_darknet.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/pfirouzabadi/.conda/envs/mmlab2/lib/python3.11/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "detector = init_detector_model(detection_cfg=detection_cfg,detection_ckpt=detection_ckpt,device='cpu')\n",
    "pose_estimator = init_pose_model(pose_model_cfg=pose_model_cfg, pose_model_ckpt=pose_model_ckpt, device=device)\n",
    "visualizer = init_visualizer(pose_estimator)\n",
    "output_root = f'./visualizations/{alg_name}/Gloves/'\n",
    "if not os.path.exists(output_root):\n",
    "    os.mkdir(output_root)\n",
    "\n",
    "for vid_file in paths:\n",
    "#RUN VIDEOS FROM SPECIFIC VIEWS\n",
    "# for vid_idx in [5]:\n",
    "# vid_file = \"/home/pfirouzabadi/projects/Hand_Detection/data/fabio_0702/m010_hand_asl_20240702_123301.23336088.mp4\"\n",
    "# vid_file_prefix=  'm010_hand_asl_20240702_123301'\n",
    "    vid_file_prefix= ''.join(vid_file.split('/')[-1].split('.')[0].split('_')[:2])\n",
    "    output_file = f'{output_root}{vid_file_prefix}.mp4'\n",
    "\n",
    "    cap = cv2.VideoCapture(vid_file)\n",
    "    frame_idx = 0\n",
    "    video_writer = None\n",
    "    pred_instances_list = []\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        frame_idx += 1\n",
    "\n",
    "        if not success:\n",
    "            break\n",
    "        # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        det_result = inference_detector(detector, frame)\n",
    "        pred_instance = det_result.pred_instances.cpu().numpy()\n",
    "        bboxes = np.concatenate(\n",
    "            (pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)\n",
    "        \n",
    "        bboxes = bboxes[np.logical_and(pred_instance.labels == 0,\n",
    "                                        pred_instance.scores > .2)]\n",
    "        bboxes = bboxes[nms(bboxes, .2), :4]\n",
    "        # bboxes[:,:2] -= 50\n",
    "        # bboxes[:,-2:] += 50\n",
    "\n",
    "        \n",
    "        # predict keypoints\n",
    "        pose_results = inference_topdown(pose_estimator, frame, bboxes)\n",
    "        data_samples = merge_data_samples(pose_results)\n",
    "\n",
    "        visualizer.add_datasample(\n",
    "            'result',\n",
    "            frame,\n",
    "            data_sample=data_samples,\n",
    "            draw_gt=False,\n",
    "            draw_heatmap=True,\n",
    "            draw_bbox=True,\n",
    "            show_kpt_idx=True,\n",
    "            # show=args.show,\n",
    "            wait_time=0,\n",
    "            kpt_thr=0.3)\n",
    "        # topdown pose estimation\n",
    "        pred_instances = data_samples.get('pred_instances', None)\n",
    "\n",
    "        # save prediction results\n",
    "        pred_instances_list.append(\n",
    "            dict(\n",
    "                frame_id=frame_idx,\n",
    "                instances=split_instances(pred_instances)))\n",
    "\n",
    "        # output videos\n",
    "        frame_vis = visualizer.get_image()\n",
    "\n",
    "        if video_writer is None:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            # the size of the image with visualization may vary\n",
    "            # depending on the presence of heatmaps\n",
    "            video_writer = cv2.VideoWriter(\n",
    "                output_file,\n",
    "                fourcc,\n",
    "                25,  # saved fps\n",
    "                (frame_vis.shape[1], frame_vis.shape[0]))\n",
    "        if frame_idx%210 == 0:\n",
    "            ress=cv2.imwrite(f'{output_file[:-4]}_frame{frame_idx}.jpeg',frame_vis)\n",
    "\n",
    "        video_writer.write(frame_vis)\n",
    "\n",
    "        # if args.show:\n",
    "        #     # press ESC to exit\n",
    "        #     if cv2.waitKey(5) & 0xFF == 27:\n",
    "        #         break\n",
    "\n",
    "        #     time.sleep(args.show_interval)\n",
    "\n",
    "    if video_writer:\n",
    "        video_writer.release()\n",
    "\n",
    "    cap.release()\n",
    "    # os.remove(vid_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Code for pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b533e3-5815-4db1-93f8-0bec7156ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/open-mmlab/mmpose/blob/dev-1.x/configs/hand_2d_keypoint/topdown_heatmap/freihand2d/td-hm_res50_8xb64-100e_freihand2d-224x224.py\n",
    "# Detection model: https://download.openmmlab.com/mmpose/mmdet_pretrained/cascade_rcnn_x101_64x4d_fpn_20e_onehand10k-dac19597_20201030.pth\n",
    "#RTMDET : https://download.openmmlab.com/mmpose/v1/projects/rtmposev1/rtmdet_nano_8xb32-300e_hand-267f9c8f.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8b20113-8d43-4a83-b035-795f7eac7edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://dd.openmmlab.com/mmpose/hand/resnet/res50_freihand_224x224-ff0799bc_20200914.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# init model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43minit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/notebooks/mmpose/mmpose/apis/inference.py:129\u001b[0m, in \u001b[0;36minit_model\u001b[0;34m(config, checkpoint, device, cfg_options)\u001b[0m\n\u001b[1;32m    126\u001b[0m model\u001b[38;5;241m.\u001b[39mdataset_meta \u001b[38;5;241m=\u001b[39m dataset_meta\n\u001b[1;32m    128\u001b[0m model\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m config  \u001b[38;5;66;03m# save the config in the model for convenience\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.conda/envs/mmlab/lib/python3.9/site-packages/mmengine/model/base_model/base_model.py:208\u001b[0m, in \u001b[0;36mBaseModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(torch\u001b[38;5;241m.\u001b[39mdevice(device))\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mmlab/lib/python3.9/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mmlab/lib/python3.9/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mmlab/lib/python3.9/site-packages/torch/nn/modules/module.py:857\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 857\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[key] \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/mmlab/lib/python3.9/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from mmcv.image import imread\n",
    "\n",
    "from mmpose.apis import inference_topdown, init_model,visualize\n",
    "from mmpose.registry import VISUALIZERS\n",
    "from mmpose.structures import merge_data_samples\n",
    "\n",
    "model_cfg = 'configs/hand_2d_keypoint/topdown_heatmap/freihand2d/td-hm_res50_8xb64-100e_freihand2d-224x224.py'\n",
    "\n",
    "ckpt = 'https://dd.openmmlab.com/mmpose/hand/resnet/res50_freihand_224x224-ff0799bc_20200914.pth'\n",
    "\n",
    "device = 'cuda:4'\n",
    "\n",
    "# init model\n",
    "model = init_model(model_cfg, ckpt, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38b0a6fc-8ec5-4838-b610-200a19a626fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtests/data/onehand10k/784.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# inference on a single image\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m batch_results \u001b[38;5;241m=\u001b[39m inference_topdown(\u001b[43mmodel\u001b[49m, img_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "img_path = 'tests/data/onehand10k/784.jpg'\n",
    "\n",
    "# inference on a single image\n",
    "batch_results = inference_topdown(model, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b29e32f-b3b5-4898-b319-ff172006a1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[143.14285, 593.1429 ],\n",
       "        [191.14285, 497.14285],\n",
       "        [239.14285, 428.57144],\n",
       "        [300.85715, 394.2857 ],\n",
       "        [390.     , 360.     ],\n",
       "        [204.85715, 380.57144],\n",
       "        [266.57144, 346.2857 ],\n",
       "        [328.2857 , 339.42856],\n",
       "        [396.85715, 339.42856],\n",
       "        [232.28572, 414.85715],\n",
       "        [294.     , 394.2857 ],\n",
       "        [355.7143 , 394.2857 ],\n",
       "        [410.57144, 401.14285],\n",
       "        [246.     , 483.42856],\n",
       "        [294.     , 462.85715],\n",
       "        [307.7143 , 483.42856],\n",
       "        [300.85715, 510.85715],\n",
       "        [266.57144, 531.4286 ],\n",
       "        [294.     , 517.7143 ],\n",
       "        [287.14285, 531.4286 ],\n",
       "        [266.57144, 545.1429 ]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_instances = batch_results[0].pred_instances\n",
    "\n",
    "pred_instances.keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab85842b-fbbc-4d97-a051-2b08cdd4f7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'PoseLocalVisualizer',\n",
       " 'vis_backends': [{'type': 'LocalVisBackend'}],\n",
       " 'name': 'visualizer'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cfg.visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce6cea99-0cda-4859-9871-70b0c8f5d9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0, 255,   0],\n",
       "        [  0, 255,   0],\n",
       "        [  0, 255,   0],\n",
       "        ...,\n",
       "        [  0, 255,   0],\n",
       "        [  0, 255,   0],\n",
       "        [  0, 255,   0]],\n",
       "\n",
       "       [[  0, 255,   0],\n",
       "        [ 39,  27,  27],\n",
       "        [ 35,  23,  23],\n",
       "        ...,\n",
       "        [132, 104,  65],\n",
       "        [132, 104,  67],\n",
       "        [133, 105,  68]],\n",
       "\n",
       "       [[  0, 255,   0],\n",
       "        [ 35,  23,  23],\n",
       "        [ 32,  20,  20],\n",
       "        ...,\n",
       "        [129, 104,  64],\n",
       "        [131, 105,  68],\n",
       "        [132, 106,  69]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0, 255,   0],\n",
       "        [165, 173, 150],\n",
       "        [165, 173, 150],\n",
       "        ...,\n",
       "        [193, 194, 176],\n",
       "        [195, 194, 176],\n",
       "        [194, 193, 175]],\n",
       "\n",
       "       [[  0, 255,   0],\n",
       "        [161, 169, 146],\n",
       "        [160, 168, 145],\n",
       "        ...,\n",
       "        [192, 193, 175],\n",
       "        [194, 193, 175],\n",
       "        [194, 193, 175]],\n",
       "\n",
       "       [[  0, 255,   0],\n",
       "        [155, 163, 140],\n",
       "        [153, 161, 138],\n",
       "        ...,\n",
       "        [190, 191, 173],\n",
       "        [193, 192, 174],\n",
       "        [194, 193, 175]]], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizer = VISUALIZERS.build(model.cfg.visualizer)\n",
    "pred_instances = batch_results[0].pred_instances\n",
    "keypoints = pred_instances.keypoints\n",
    "keypoint_scores = pred_instances.keypoint_scores\n",
    "img = imread(img_path, channel_order='rgb')\n",
    "results = merge_data_samples(batch_results)\n",
    "# metainfo = 'configs/_base_/datasets/onehand10k.py'\n",
    "visualizer.add_datasample(\n",
    "            'result',\n",
    "            img,\n",
    "            data_sample=results,\n",
    "            # det_data_sample=data_samples_2d,\n",
    "            draw_gt=False,\n",
    "            draw_bbox=True,\n",
    "            kpt_thr=0.3,\n",
    "            draw_heatmap=True,\n",
    "            # show_kpt_idx=args.show_kpt_idx,\n",
    "            # skeleton_style=args.skeleton_style,\n",
    "            show=False,out_file='a.jpg',\n",
    "            wait_time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3592f881-d9ea-41fe-b85b-506a46a062b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mmpose.apis import visualize\n",
    "\n",
    "# pred_instances = batch_results[0].pred_instances\n",
    "\n",
    "# keypoints = pred_instances.keypoints\n",
    "# keypoint_scores = pred_instances.keypoint_scores\n",
    "\n",
    "# metainfo = 'configs/_base_/datasets/coco_wholebody_hand.py'#'config/_base_/datasets/crowdpose.py'\n",
    "# # configs/_base_/datasets/coco_wholebody_openpose.py\n",
    "# visualize(\n",
    "#     img_path,\n",
    "#     keypoints,\n",
    "#     keypoint_scores,\n",
    "#     metainfo=metainfo,\n",
    "#     # show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b077dd7-20d9-4999-a67e-3a9a25dea139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge results as a single data sample\n",
    "# # results = merge_data_samples(batch_results)\n",
    "\n",
    "# # build the visualizer\n",
    "# visualizer = VISUALIZERS.build(model.cfg.visualizer)\n",
    "\n",
    "# # set skeleton, colormap and joint connection rule\n",
    "# # visualizer.set_dataset_meta(model.dataset_meta)\n",
    "\n",
    "# img = imread(img_path, channel_order='rgb')\n",
    "\n",
    "# # visualize the results\n",
    "# visualizer.add_datasample(\n",
    "#     'result',\n",
    "#     img,\n",
    "#     data_sample=batch_results,\n",
    "#     show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5bb724-093a-4914-80eb-ef85a179f2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
